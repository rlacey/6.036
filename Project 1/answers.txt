1. Using an offset did not reduce training error of the averager classifier. In fact it increasing the number of misclassifications.

3. One set of data has wide margins and while the other set has a small margin between two oppositely labeled points. The first graph is easily separable and each classifier is able to find a linear boundary for the data, which could lead to the false assumption that they work approximately the same. The second data puts this idea to rest. The average completely fails to separate the data. The perceptron result finds a decision boundary, but it nearly lies on top of one of the data points. The passive-agressive algorithm not only finds a separator, but finds one with nearly equidistant margins from the nearest oppositely labeled points.

4. I ran cross validation across each of the three algorithms with

5. Machine learning classification lets a computer make educated guesses about the type of a thing. For this project the thing we were studying was "tweets" and the types were "positive" and "negative", describing the dispoition of the tweet. To make these guesses, the computer reads a lot of tweets which it knows are positive or negative and learns what words in them make the tweets that way. For example, if the word "terrific" was almost exclusively in tweets that were posiitve, then if it saw that word in a new tweet it would make an educated guess that the new tweet is a positive one. Thus the computer has in a sense learned the disposition of various words.

6. My function extract_feature_vectors_with_keywords modifies the original feautre set by weighting features. I provide a text file 'adjectives.txt' that contains a list of adjectives, adverbs, etc. When building the feature matrix I maintain the behavior of giving value 0 to words not in the tweet. For words in the tweet, however, if it is one of the adjective words then it will receive a value of 2 and otherwise receive a value of 1 as before. The goal was to give words that are likely to express dispoition (good, terrible, best, etc.) greater influence in classification. I ran cross validation on the original feature matrix with a correct average classification of 83.2% for the passive-agrssive algorithm and on my modified feature matric with a correct average classification of 84.9% for the same algorithm. Thus performance improved using my new feature set.
