%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% University/School Laboratory Report
% LaTeX Template
% Version 3.0 (4/2/13)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Linux and Unix Users Group at Virginia Tech Wiki 
% (https://vtluug.org/wiki/Example_LaTeX_chem_lab_report)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}

\usepackage[version=3]{mhchem} % Package for chemical equation typesetting
\usepackage{siunitx} % Provides the \SI{}{} command for typesetting SI units

\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{float}

\usepackage[T1]{fontenc} % allow small bold caps

\setlength\parindent{0pt} % Removes all indentation from paragraphs

\renewcommand{\labelenumi}{\alph{enumi}.} % Make numbering in the enumerate environment by letter rather than number (e.g. section 6)

\usepackage[margin=1in]{geometry}

\usepackage{amssymb}

%\usepackage{times} % Uncomment to use the Times New Roman font

%----------------------------------------------------------------------------------------
%	Title
%----------------------------------------------------------------------------------------

\begin{document}
\pagenumbering{gobble}

\title{6.036: Machine Learning}
\author{
  Ryan Lacey <rlacey@mit.edu>\\
  \footnotesize \texttt{Collaborator(s): Jorge Perez, Amruth Venkatraman}
}
        
\maketitle
        


\begin{enumerate}
\item[6.]
	\begin{enumerate}
	\item[(c)] 
		        The range of possible separators is much smaller in part (b) than in part (a), which increases the likelihood of the perceptron algorithm requiring a high number of iterations. Starting with $x^{(2)}$ we do not see this issue because the resulting decision boundary immediately and perfectly separates the data points. Starting with $x^{(1)}$, on the other hand, we see how the smaller changes in the slope of the decision boundary with $x^{(3)}$ at [10, 1] has a large impact on the performance of the algorithm, taking three times as many iterations as with $x^{(3)}$ at [1.5, 1].
	\item[(d)] 
		        An adversary maximizes potential errors with the following strategy (maximizing $\frac{R^2}{\gamma^2}$):
		        \begin{itemize}
		          \item Spread the points out over a large area, thereby increasing $R$.
		          \item Assign labels to points in a manner that minimizes the margin between differently labeled points, thereby reducing $\gamma$. 
		          \item Traverse the points in order from nearest to the origin to furthest. Smaller magnitude vectors will require more mistakes to be made in order to converge to a decision boundary because of their small incremental progress.
		        \end{itemize}
	\end{enumerate}

\bigskip

\item[7.]
	\begin{enumerate}
	\item 
		  No, the training procedures do not necessarily converge to the same $\theta$ and $\theta_0$. If training points are linearly separable, there are many choices for $\theta$ and $\theta_0$.
		  \newline\newline
		  As an example take the training points $x^{(1)}=[2,1]$, $x^{(2)}=[-2,-1]$, and $x^{(3)}=[-1,5]$ with corresponding labels $y^{(1)}=(+1)$, $y^{(2)}=(-1)$, and $y^{(3)}=(+1)$.
		  \newline\newline
		  We define two different decisions boundaries with the following $\theta$ and $\theta_0$:
		  \newline\newline
		  $\theta^{(1)} = \left[ \begin{smallmatrix} 0\\1 \end{smallmatrix} \right]$ and $\theta^{(1)}_{0} = 0$
		  \newline\newline
		  $\theta^{(2)} = \left[ \begin{smallmatrix} 1\\1 \end{smallmatrix} \right]$ and $\theta^{(2)}_{0} = -1$
		  \newline\newline
		  These decision boundaries linearly separate the data so when the perceptron algorithm is applied to the training data with these parameters the boundaries incur no updates. Thus we have differing results from the perceptron algorithm on the same data.
	\item 
		  The decision boundaries will not necessarily have the same performance on the test data. Let us assume we have some test data that is linearly separable along the $x_1$ axis. In this case the first decision 		boundary from part (a) would perfectly classify the data, while the second boundary is apt to incorrectly predict several points.  
	\item 
		   $\theta = \left[ \begin{smallmatrix} -3\\2 \end{smallmatrix} \right]$ and $\theta_0 = -2$
	\end{enumerate}

\bigskip

\item[8.]
	\begin{enumerate}
	\item
		\begin{enumerate}
		\item No such $\theta$ could exist because the dot product with $x$ would always be zero, thus making the second constraint unsatisfiable.
		\item Yes, the addition of the offset parameter would allow us to satisfy both constraints.
		\end{enumerate}
	\item
		\begin{enumerate}
		\item 
			  No such family member exists because three of the points are equidistant from the origin and only two are of the same label. Thus the classifier would have to predict these points to all be of the same value, which is incorrect.
		\item 
			  The points can be correctly classified with a circle of radius 2 centered at [-1, -1].
		\item 
			  Looking at only the negatively labeled points we can conclude that no such family member exists. The points would either lie directly on the decision boundary (thus both being misclassified) or one would lie above and the other below the boundary (thus misclassifying one of the two points).
		\item 
			  A line with normal [1,1] and offset of -0.5 would satisfy.
		\end{enumerate}
	\item Families (iii) and (iv) are linear classifiers.
	\end{enumerate}

\bigskip

\item[9.]
	\begin{enumerate}
	\item
		  $A = \left[ \begin{smallmatrix} 
		       \frac{1}{6} & \frac{1}{6} & \frac{1}{6} & \frac{1}{6} & \frac{1}{6} & \frac{1}{6}\\
		       \frac{1}{3} & \frac{1}{3} & \frac{1}{3} &-\frac{1}{3} &-\frac{1}{3} &-\frac{1}{3} \end{smallmatrix} \right]$
	\item 
		  Yes, given a good $\theta_z$ one can find an equivalent $\theta_x$.
		  \newline\newline
		  Classifier in $z$-space $y^{(i)}\left(\theta_z z^{(i)}\right) > 0$ becomes $y^{(i)}\left(\theta_z A x^{(i)}\right) > 0$.
		  \newline\newline
		  We desire the $\theta_x$, which must equal $\theta_z A$ from the above equation, which gives the familiar $y^{(i)}\left(\theta_x x^{(i)}\right) > 0$.
	\item 
		  No, there will not necessarily always be a $\theta_z$ that will identically classify all the $z$'s. It will only hold when $A$ is invertible.
		  \newline\newline
		  Classifier $y^{(i)}\left(\theta_x x^{(i)}\right) > 0$ becomes $y^{(i)}\left(\theta_x A^{-1} z^{(i)}\right) > 0$ which requires A to be invertible. 
	\item 
		  Yes, the algorithm can converge more quickly in $z$-space. Take the situation in which one feature is enough to classify the data. Then the goal is only to converge over this feature rather than a large dimensional vector, which should require less iterations of the algorithm.
	\item 
		  No, one cannot find a more accurate classifier in $z$-space on the training data because you have at best the same amount of information to train on as you would in $x$-space. You could have overtraining on the training data, however, in which case the classifier in $z$-space could outperform the classifier in $x$-space on test data.
	\end{enumerate}

\bigskip

\item[10.]
	\begin{enumerate}
	\item 
		  The training set might not be representative of the general population of data. If so, then the classifier would be a poor predictor of the correct labels that should be applied to the population data.
		  \newline\newline
		  Only measuring performance by the training set also leads to a tendency of overfitting to the training data. Because of this, even with training data that is fairly representative of population data, the predictions may not generalize well.
	\item 
		  The perceptron algorithm would be ran on $n-1$ points $n$ number of times, each time leaving out one of the training points to use as the test. The goal of cross validation is to test how well the classifier generalizes and about how often it is expected to misclassify data from the test set. If the training data is drawn from and is representative of the population being studied, then cross validation should show errors about as often as test set validation. The advantage, however, is that cross validation requires less data to be collected since data points are reused from training for testing. 
	\item 
		  A large number of cross validation errors with a low number of training set errors suggests that the boundary decision resulting from the learning algorithm overfit to the training set. One possible solution would be to take the best decision boundary that resulted from the data points subsets of the cross validation runs. 
	\end{enumerate}

\end{enumerate}

\end{document}